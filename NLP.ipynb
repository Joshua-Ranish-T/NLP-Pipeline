{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu5hiiS5VkGEZoj1yKysBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joshua-Ranish-T/NLP-Pipeline/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing Pipeline**\n"
      ],
      "metadata": {
        "id": "4r-M5fTJyPHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Acquisition\n",
        "Extraction of data"
      ],
      "metadata": {
        "id": "lLFfG56VyrI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning\n",
        "\n",
        "removing unnecessary stuffs like html tag,etc with reqex,unicode"
      ],
      "metadata": {
        "id": "gweGsfdgy4Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"\"\"<gfg>\n",
        "#GFG Geeks Learning together\n",
        "url <https://www.geeksforgeeks.org/>,\n",
        "email <acs@sdf.dv>\n",
        "\"\"\"\n",
        "\n",
        "html = re.compile('[<,#*?>]')\n",
        "text = html.sub(r'',text)\n",
        "url = re.compile('https?://\\S+|www\\.S+')\n",
        "text = url.sub(r'',text)\n",
        "email = re.compile('[A-Za-z0-2]+@[\\w]+.[\\w]+')\n",
        "text = email.sub(r'',text)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En9BQGaozKk7",
        "outputId": "d5e038c4-1ff5-4da2-96b6-711535072f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gfg \n",
            "GFG Geeks Learning together \n",
            "url  \n",
            "email \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "ztGXoKLi1eaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization - breaks text into smaller chunks known as tokens"
      ],
      "metadata": {
        "id": "89eudfw917XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download both punkt and punkt_tab\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = 'GeeksforGeeks is a very famous edutech company in the IT industry.'\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBBlpcrI19FY",
        "outputId": "d508e73b-b46c-4cd0-9e99-9b01b47633c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "['GeeksforGeeks', 'is', 'a', 'very', 'famous', 'edutech', 'company', 'in', 'the', 'IT', 'industry', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lowercasing"
      ],
      "metadata": {
        "id": "6xTU3cF23CPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texter = [i.lower() for i in tokens]\n",
        "texter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5cStPwo3hnZ",
        "outputId": "02eb565f-20f3-4dd4-b4cf-bd398d843cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['geeksforgeeks',\n",
              " 'is',\n",
              " 'a',\n",
              " 'very',\n",
              " 'famous',\n",
              " 'edutech',\n",
              " 'company',\n",
              " 'in',\n",
              " 'the',\n",
              " 'it',\n",
              " 'industry',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stop word Removal"
      ],
      "metadata": {
        "id": "Nz9aWMT-35fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in texter if token not in stop_words]\n",
        "filtered_tokens\n",
        "# stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1PQNwr_4FDu",
        "outputId": "e92e49ba-0345-43aa-a26f-4564b7dee48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['geeksforgeeks', 'famous', 'edutech', 'company', 'industry', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove digit/puntuation"
      ],
      "metadata": {
        "id": "N6sZyPkC5C36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "cleaned_tokens = [token for token in filtered_tokens\n",
        "                  if not token.isdigit() and not token in string.punctuation]\n",
        "cleaned_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bba4x5eL6ExW",
        "outputId": "f1c3b82d-f951-4926-f55b-0df8c8c09c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['geeksforgeeks', 'famous', 'edutech', 'company', 'industry']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "converts to base form by removing suffix term like runner -> run"
      ],
      "metadata": {
        "id": "_AjcizX06yRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "# stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
        "# stemmed_tokens\n",
        "print(stemmer.stem('beautiful'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D-7LEYR697X",
        "outputId": "79f8703e-e191-4aa8-8fbf-2fa61c15b0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beauti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmitization\n",
        "\n",
        "converts to base form using parts of speech like better to good (good,better,best)"
      ],
      "metadata": {
        "id": "FlJfs5ns7ck2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download the wordnet resource for lemmatization\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
        "lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np2Qq7xX75LH",
        "outputId": "b45ed70d-1423-46dc-a951-0f73577f2779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['geeksforgeeks', 'famous', 'edutech', 'company', 'industry']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# parts of speech\n",
        "\n",
        "-Identifying the parts of speech like noun,adj,verb..."
      ],
      "metadata": {
        "id": "2xNGiFXW8bsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "import nltk # Import nltk\n",
        "\n",
        "# Download the required resources for pos_tag\n",
        "nltk.download('averaged_perceptron_tagger') # This was already there\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Add this line to download the specific resource\n",
        "\n",
        "pos_tags = pos_tag(stemmed_tokens)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ccXgvjD841Z",
        "outputId": "7bc3b25d-d7dc-4da5-cdbb-4904edd26ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('geeksforgeek', 'RB'),\n",
              " ('famous', 'JJ'),\n",
              " ('edutech', 'NN'),\n",
              " ('compani', 'NN'),\n",
              " ('industri', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition\n",
        "Identifying named entities such as person names, organizations, locations..."
      ],
      "metadata": {
        "id": "xdhI6P8a9eKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "named_entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Y0pWSkP09_s_",
        "outputId": "46c3ef99-b99b-45d0-e0e8-a2c3a4404af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('geeksforgeek', 'RB'), ('famous', 'JJ'), ('edutech', 'NN'), ('compani', 'NN'), ('industri', 'NN')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,400.0,120.0\" width=\"400px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"28%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">geeksforgeek</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16%\" x=\"28%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">famous</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36%\" y1=\"20px\" y2=\"48px\" /><svg width=\"18%\" x=\"44%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">edutech</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53%\" y1=\"20px\" y2=\"48px\" /><svg width=\"18%\" x=\"62%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">compani</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71%\" y1=\"20px\" y2=\"48px\" /><svg width=\"20%\" x=\"80%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">industri</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**\n",
        "converting to vector"
      ],
      "metadata": {
        "id": "x_pqSJkY-7jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# one hot encoder"
      ],
      "metadata": {
        "id": "wB9J638D_FTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categories = ['Red', 'Green', 'Blue', 'Red', 'Green']\n",
        "\n",
        "# Reshape the data to be a 2D array (required by OneHotEncoder)\n",
        "categories_reshaped = np.array(categories).reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "one_hot_encoded_data = encoder.fit_transform(categories_reshaped)\n",
        "\n",
        "print(\"Original data:\", categories)\n",
        "print(\"One-hot encoded data:\\n\", one_hot_encoded_data)\n",
        "\n",
        "print(\"Categories:\", encoder.categories_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaH3a1lP_bKk",
        "outputId": "77e2f206-8bc3-4815-e961-7fc57df373e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data: ['Red', 'Green', 'Blue', 'Red', 'Green']\n",
            "One-hot encoded data:\n",
            " [[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n",
            "Categories: [array(['Blue', 'Green', 'Red'], dtype='<U5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words"
      ],
      "metadata": {
        "id": "RqUeGlC0_bzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third document.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "bag_of_words_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the feature names and the Bag of Words matrix\n",
        "print(\"Feature names:\", feature_names)\n",
        "print(\"Bag of Words Matrix:\\n\", bag_of_words_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuwhj1_h_kYy",
        "outputId": "1e643b95-1a7b-4c40-a90b-b35956c73d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['and' 'document' 'first' 'is' 'second' 'the' 'third' 'this']\n",
            "Bag of Words Matrix:\n",
            " [[0 1 1 1 0 1 0 1]\n",
            " [0 2 0 1 1 1 0 1]\n",
            " [1 1 0 1 0 1 1 1]\n",
            " [0 1 1 1 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency - Inverse Document Frequency\n",
        "\n",
        "\n",
        "*   TF means probablity of finding the word in the sentence.\n",
        "*   idf means log(Tot no of sentences / no.of sentences containing the word).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUJJ6cM4_pgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third document.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the TF-IDF matrix and feature names\n",
        "print(\"Feature names:\", feature_names)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray()) #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JtgH3ds_1IC",
        "outputId": "53c42250-e9d4-4032-951d-3e34be171fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['and' 'document' 'first' 'is' 'second' 'the' 'third' 'this']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.39896105 0.60276058 0.39896105 0.         0.39896105\n",
            "  0.         0.39896105]\n",
            " [0.         0.61221452 0.         0.30610726 0.5865905  0.30610726\n",
            "  0.         0.30610726]\n",
            " [0.56894695 0.29690012 0.         0.29690012 0.         0.29690012\n",
            "  0.56894695 0.29690012]\n",
            " [0.         0.39896105 0.60276058 0.39896105 0.         0.39896105\n",
            "  0.         0.39896105]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fbUWa_SL_10S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word embeddings**\n",
        "\n"
      ],
      "metadata": {
        "id": "t647gL-M_-ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cbow - Continuous Bag Of Words\n",
        "predict target value by its surrounding words"
      ],
      "metadata": {
        "id": "klEXidkeATxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# skipgram\n",
        "\n",
        "predict surrounding words from target word"
      ],
      "metadata": {
        "id": "M-obR9cfAqwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre-trained word embeddings\n",
        "\n",
        "Word2vec ,etc.."
      ],
      "metadata": {
        "id": "15xkGmfvBFLd"
      }
    }
  ]
}